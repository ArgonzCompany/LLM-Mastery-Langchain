{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nPQguxAiSKyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a54a21-fcda-485d-dbf6-a12e59e3aff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q sentence-transformers\n",
        "!pip install -q langchain langchain_community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziKLSdc0xIw7",
        "outputId": "5e8c58f3-18e6-419b-8e20-0d8d1489fc02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -q localtunnel"
      ],
      "metadata": {
        "id": "htVN_CqFS7zv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23640c0-9c69-4374-d096-d03f4b9dfa76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "KLYxxCtajaRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2e2cce-2d18-4755-b5f3-1c43b8ca9402"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ2NXGqqbYf5",
        "outputId": "d39957e0-e2ae-4535-d545-93cb8d725e3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "il48yyqUbp4S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WC9reZxby1B",
        "outputId": "1f351d70-8e18-490d-efdb-636424f86832"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m297.0/298.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "HUGGINGFACE_API_KEY=\"Your API Key\"\n",
        "HUGGINGFACEHUB_API_KEY=\"Your API Key\"\n",
        "HF_TOKEN=\"Your API Key\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOWMqKif-g0E",
        "outputId": "b5af0c4f-e957-4b5e-ad73-8e4e27ea752b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "# login(\"Your API Key\")"
      ],
      "metadata": {
        "id": "xSOTlDp3r2qz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "G9oUBu6KB9Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f7de52-ce4e-4c67-f224-93beddfa417a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatdocument.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "import faiss\n",
        "import tempfile\n",
        "import os\n",
        "import time\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "#Streamlit setting\n",
        "st.set_page_config(page_title=\"Chat with Documents 📚\", page_icon=\"📚\")\n",
        "st.title(\"Chat with Documents 📚\")\n",
        "\n",
        "\n",
        "def model_hf_hub(model=\"microsoft/Phi-3.5-mini-instruct\", temperature=0.1):\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id = model,\n",
        "        temperature = temperature,\n",
        "        max_new_tokens = 512,\n",
        "        return_full_text = False,\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Indexing and retrieval\n",
        "def config_retrieval(uploads):\n",
        "    #Load\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploads:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    #Split\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap = 200\n",
        "    )\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    #Embedding\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "    #Store\n",
        "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "    vectorstore.save_local('vectorstore/db_faiss')\n",
        "\n",
        "    #Retrieve\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type = \"mmr\",\n",
        "        search_kwargs = {\n",
        "            'k': 3,\n",
        "            'fetch_k': 4\n",
        "        }\n",
        "        ) #Maximum Marginal Relevant (mmr)\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def config_rag_chain(retriever):\n",
        "    llm = model_hf_hub()\n",
        "    token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "\n",
        "    #query -> retriever\n",
        "    #(query, chat_history) -> LLM -> reformulated query -> retriever\n",
        "    #Contextualization Prompt\n",
        "    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
        "    context_q_system_prompt = token_s + context_q_system_prompt\n",
        "    context_q_user_prompt = \"Question: {input}\" + token_e\n",
        "    context_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            ('system', context_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            ('human', context_q_user_prompt)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    #Chain for contextualization\n",
        "    history_aware_retriever = create_history_aware_retriever(llm = llm, retriever = retriever, prompt = context_q_prompt)\n",
        "\n",
        "    #Q&A Prompt\n",
        "    qa_prompt_template = \"\"\"You are a helpful virtual assistant answering general questions.\n",
        "    Use the following bits of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say you don't know. keep your answer concise.\n",
        "    Answer in English. \\n\\n\n",
        "    Question: {input} \\n\n",
        "    Context: {context}\"\"\"\n",
        "\n",
        "    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n",
        "\n",
        "    #Configure LLM and Chain for Q&A\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "\n",
        "#Create side panel in interface\n",
        "uploads = st.sidebar.file_uploader(\n",
        "    label = \"Upload files\",\n",
        "    type = [\"pdf\"],\n",
        "    accept_multiple_files = True\n",
        ")\n",
        "\n",
        "if not uploads:\n",
        "    st.info(\"Please send some files to continue\")\n",
        "    st.stop()\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = [\n",
        "        AIMessage(content = \"Hi, I'm your virtual assistant! How can I help you?\")\n",
        "    ]\n",
        "\n",
        "if \"docs_list\" not in st.session_state:\n",
        "    st.session_state.docs_list = None\n",
        "\n",
        "if \"retriever\" not in st.session_state:\n",
        "    st.session_state.retriever = None\n",
        "\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)\n",
        "\n",
        "user_query = st.chat_input(\"Enter your message here...\")\n",
        "\n",
        "if user_query is not None and user_query != \"\" and uploads is not None:\n",
        "    st.session_state.chat_history.append(HumanMessage(content = user_query))\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "    with st.chat_message(\"AI\"):\n",
        "        if st.session_state.docs_list != uploads:\n",
        "            st.session_state.docs_list = uploads\n",
        "            st.session_state.retriever = config_retrieval(uploads)\n",
        "        rag_chain = config_rag_chain(st.session_state.retriever)\n",
        "        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n",
        "\n",
        "        resp = result['answer']\n",
        "        st.write(resp)\n",
        "\n",
        "\n",
        "        #Show the source\n",
        "        sources = result['context']\n",
        "        for idx, doc in enumerate(sources):\n",
        "            source = doc.metadata['source']\n",
        "            file = os.path.basename(source)\n",
        "            page = doc.metadata.get('page', 'Page not specified')\n",
        "            ref = f\":link: Source {idx}: *{file} - p. {page}*\"\n",
        "            with st.popover(ref):\n",
        "                st.caption(doc.page_content)\n",
        "    st.session_state.chat_history.append(AIMessage(content = resp))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU1Sy4HYCBGd",
        "outputId": "b84b9ead-6ddb-49c3-936b-4e3666e663f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chatdocument.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run chatdocument.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "fclhvqrNCorM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2LBxEFHDbhM",
        "outputId": "c5bb9726-d939-4a3f-f360-247e0f47985d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.82.200.229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "hERI-wD_DSSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28ed6e6-24ca-463f-a69d-149666bb5d55"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://quiet-lines-type.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}