{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nPQguxAiSKyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d8020c-9129-451b-968c-4dc238f094dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit sentence-transformers\n",
        "!pip install -q langchain langchain_community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "htVN_CqFS7zv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de44266c-4c12-4249-f684-632be4539f00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "KLYxxCtajaRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4bccb6-be40-419a-a17a-0a1c222758f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "HUGGINGFACE_API_KEY=\"Your API Key\"\n",
        "HUGGINGFACEHUB_API_KEY=\"Your API Key\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOWMqKif-g0E",
        "outputId": "c26c73dd-fd4d-4fe2-96af-647098ae8cce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "G9oUBu6KB9Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be074d26-0870-48b4-c11c-fc60aa9e2b9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbotproj.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "#Streamlit setting\n",
        "st.set_page_config(page_title=\"Your AI Assistant🤖\", page_icon=\"🤖\")\n",
        "st.title(\"Your AI Assistant 🤖\")\n",
        "# st.button(\"Test\")\n",
        "# st.chat_input()\n",
        "\n",
        "def model_hf_hub(model=\"mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated\", temperature=0.1):\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id = model,\n",
        "        temperature = temperature,\n",
        "        max_new_tokens = 512,\n",
        "        return_full_text = False,\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "def model_response(user_query, chat_history):\n",
        "    llm = model_hf_hub()\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You are a helpful assistant answering general questions. Please respond in {langiage}.\n",
        "    \"\"\"\n",
        "    language = \"the same language the user is using to chat\"\n",
        "    user_prompt = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "    prompt_template = ChatPromptTemplate.from_messages(\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", user_prompt)\n",
        "    )\n",
        "\n",
        "    #Creating the chain\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    #Response\n",
        "    return chain.stream({\n",
        "      \"chat_history\": chat_history,\n",
        "      \"input\": user_query,\n",
        "      \"language\": language\n",
        "    })\n",
        "\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = [\n",
        "        AIMessage(content = \"Hi, I'm your virtual assistant! How can I help you?\")\n",
        "    ]\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)\n",
        "\n",
        "user_query = st.chat_input(\"Enter your message here ...\")\n",
        "if user_query is not None and user_query != \"\":\n",
        "    st.session_state.chat_history.append(HumanMessage(content = user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "        resp = st.write_stream(model_response(user_query, st.session_state.chat_history))\n",
        "\n",
        "    st.session_state.chat_history.append(AIMessage(content = resp))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU1Sy4HYCBGd",
        "outputId": "e41969d4-2722-4977-abfc-d48b3c26d499"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing chatbotproj.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run chatbotproj.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "fclhvqrNCorM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2LBxEFHDbhM",
        "outputId": "ad7ca415-5364-4285-e026-1128682f4d55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.204.20.137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "hERI-wD_DSSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8013a2d8-a5c8-474c-eff9-6b9eb9f6027d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://full-bushes-buy.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}